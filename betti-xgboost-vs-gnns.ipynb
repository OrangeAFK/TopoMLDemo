{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install torch\n!pip install torch-geometric                      \n!pip install ripser                               \n!pip install networkx                             \n!pip install scikit-learn                         \n!pip install gudhi\n!pip install giotto-tda\n!pip install networkx","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-08T15:44:15.373861Z","iopub.execute_input":"2025-08-08T15:44:15.375199Z","iopub.status.idle":"2025-08-08T15:44:46.300548Z","shell.execute_reply.started":"2025-08-08T15:44:15.375164Z","shell.execute_reply":"2025-08-08T15:44:46.299111Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.datasets import TUDataset\nfrom torch_geometric.loader import DataLoader\nfrom torch_geometric.nn import GCNConv, GINConv, SAGEConv, GATConv, global_mean_pool\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport xgboost as xgb\nfrom gudhi import SimplexTree\nimport networkx as nx\nfrom gtda.diagrams import BettiCurve\nimport scipy\nimport scipy.sparse.linalg\nimport random\nfrom torch_geometric.transforms import BaseTransform\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T16:07:57.726134Z","iopub.execute_input":"2025-08-08T16:07:57.726503Z","iopub.status.idle":"2025-08-08T16:07:57.732600Z","shell.execute_reply.started":"2025-08-08T16:07:57.726477Z","shell.execute_reply":"2025-08-08T16:07:57.731763Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"# ===== FILTRATION METHODS =====\n\n# helper\ndef to_networkx(data):\n    G = nx.Graph()\n    edge_index = data.edge_index.numpy()\n    edges = list(zip(edge_index[0], edge_index[1]))\n    G.add_edges_from(edges)\n    # Add isolated nodes if any (nodes with no edges)\n    for i in range(data.num_nodes):\n        if i not in G:\n            G.add_node(i)\n    return G\n\n\ndef degree_filtration(data):\n    # Use bincount with data.num_nodes for robustness\n    deg = torch.bincount(data.edge_index.flatten(), minlength=data.num_nodes)\n    return deg.numpy()\n\n\ndef clustering_coeff_filtration(data):\n    G = to_networkx(data)\n    clust = nx.clustering(G)\n    coeffs = np.array([clust.get(i, 0) for i in range(data.num_nodes)])\n    return coeffs\n\ndef pagerank_filtration(data):\n    G = to_networkx(data)\n    pr = nx.pagerank(G)\n    pr_vals = np.array([pr.get(i, 0) for i in range(data.num_nodes)])\n    return pr_vals\n\ndef heat_kernel_filtration(data, t=1.0):\n    # skip if there's too many nodes\n    if data.num_nodes > 500:\n        print(\"Skipped HKS filtration due to size\")\n        return np.zeros(data.num_nodes)\n\n    G = to_networkx(data)\n    L = nx.normalized_laplacian_matrix(G)\n    try:\n        eigvals, eigvecs = scipy.sparse.linalg.eigsh(L, k=min(100, data.num_nodes - 1), which='SM')\n    except:\n        # Fallback for small graphs or graphs with disconnected components\n        L_dense = L.todense()\n        eigvals, eigvecs = np.linalg.eigh(L_dense)\n    diag = np.sum((eigvecs ** 2) * np.exp(-t * eigvals)[None, :], axis=1)\n    return diag","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T15:44:46.311545Z","iopub.execute_input":"2025-08-08T15:44:46.311798Z","iopub.status.idle":"2025-08-08T15:44:46.337955Z","shell.execute_reply.started":"2025-08-08T15:44:46.311778Z","shell.execute_reply":"2025-08-08T15:44:46.337107Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"def build_simplex_tree(data, filtration_values):\n    st = SimplexTree()\n    for i, fval in enumerate(filtration_values):\n        st.insert([i], filtration=fval)\n    edges = data.edge_index.t().numpy()\n    for u, v in edges:\n        # Gudhi expects simplices to be sorted, so we sort the edge\n        sorted_edge = sorted([u, v])\n        fval = max(filtration_values[u], filtration_values[v])\n        st.insert(sorted_edge, filtration=fval)\n    return st\n\ndef compute_betti_curve(st, dim=0, n_bins=100):\n    st.compute_persistence()\n    diag = np.array(st.persistence_intervals_in_dimension(dim))\n\n    if diag.size > 0:\n        # Safely handle the case where all intervals are infinite\n        finite_diag = diag[np.isfinite(diag)]\n        if finite_diag.size > 0:\n            max_finite = np.max(finite_diag)\n            # Replace infinite values with a value slightly larger than the max finite value\n            diag[np.isinf(diag)] = max_finite + 1\n        else:\n            # If all intervals are infinite, set them to a large number\n            diag[np.isinf(diag)] = 1.0\n\n        dim_col = np.full((diag.shape[0], 1), fill_value=dim)\n        diag_3d = np.hstack([diag, dim_col])\n    else:\n        # Handle empty diagrams gracefully (no intervals)\n        diag_3d = np.empty((0, 3))\n    \n    bc = BettiCurve(n_bins=n_bins)\n    betti_vector = bc.fit_transform([diag_3d])[0]\n    return betti_vector\n\n\ndef compute_betti_vector(data, filtration_fn):\n    filt_vals = filtration_fn(data)\n    st = build_simplex_tree(data, filt_vals)\n    return compute_betti_curve(st, dim=0)\n\ndef get_betti_vectors(dataset, filtration_fn):\n    X, y = [], []\n    for data in dataset:\n        bv = compute_betti_vector(data, filtration_fn)\n        X.append(bv)\n        # Fix: Ensure label is always a single number\n        label = data.y.item() if data.y.dim() > 0 else data.y\n        y.append(label)\n    \n    # Check for empty X before stacking\n    if not X:\n        return np.array([]).reshape(0, n_bins), np.array([])\n        \n    X = np.vstack(X)  # ensures 2D shape for X\n    y = np.array(y)\n    return X, y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T15:44:46.339205Z","iopub.execute_input":"2025-08-08T15:44:46.339840Z","iopub.status.idle":"2025-08-08T15:44:46.358829Z","shell.execute_reply.started":"2025-08-08T15:44:46.339801Z","shell.execute_reply":"2025-08-08T15:44:46.357935Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"class GCN(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, dropout=0.5):\n        super().__init__()\n        self.convs = torch.nn.ModuleList()\n        # Input layer\n        self.convs.append(GCNConv(in_channels, hidden_channels))\n        # Hidden layers\n        for _ in range(num_layers - 2):\n            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n        # Output layer\n        if num_layers > 1:\n            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n        \n        self.lin = torch.nn.Linear(hidden_channels, out_channels)\n        self.dropout = dropout\n\n    def forward(self, data):\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n        for conv in self.convs[:-1]:\n            x = conv(x, edge_index)\n            x = F.relu(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        \n        x = self.convs[-1](x, edge_index)\n        x = F.relu(x)\n        x = global_mean_pool(x, batch)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.lin(x)\n        return x\n\nclass GIN(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, dropout=0.5):\n        super().__init__()\n        self.convs = torch.nn.ModuleList()\n        for layer in range(num_layers):\n            if layer == 0:\n                nn = torch.nn.Sequential(\n                    torch.nn.Linear(in_channels, hidden_channels),\n                    torch.nn.ReLU(),\n                    torch.nn.Linear(hidden_channels, hidden_channels),\n                )\n            else:\n                nn = torch.nn.Sequential(\n                    torch.nn.Linear(hidden_channels, hidden_channels),\n                    torch.nn.ReLU(),\n                    torch.nn.Linear(hidden_channels, hidden_channels),\n                )\n            self.convs.append(GINConv(nn))\n        \n        self.lin = torch.nn.Linear(hidden_channels, out_channels)\n        self.dropout = dropout\n\n    def forward(self, data):\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n        for conv in self.convs[:-1]:\n            x = conv(x, edge_index)\n            x = F.relu(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n            \n        x = self.convs[-1](x, edge_index)\n        x = F.relu(x)\n        x = global_mean_pool(x, batch)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.lin(x)\n        return x\n\nclass GraphSAGE(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, dropout=0.5):\n        super().__init__()\n        self.convs = torch.nn.ModuleList()\n        # Input layer\n        self.convs.append(SAGEConv(in_channels, hidden_channels))\n        # Hidden layers\n        for _ in range(num_layers - 2):\n            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n        # Output layer\n        if num_layers > 1:\n            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n\n        self.lin = torch.nn.Linear(hidden_channels, out_channels)\n        self.dropout = dropout\n\n    def forward(self, data):\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n        for conv in self.convs[:-1]:\n            x = conv(x, edge_index)\n            x = F.relu(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n            \n        x = self.convs[-1](x, edge_index)\n        x = F.relu(x)\n        x = global_mean_pool(x, batch)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.lin(x)\n        return x\n\nclass GAT(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, heads=4, dropout=0.6):\n        super().__init__()\n        self.convs = torch.nn.ModuleList()\n        # Input layer\n        self.convs.append(GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout))\n        # Hidden layers\n        for _ in range(num_layers - 2):\n            self.convs.append(GATConv(hidden_channels * heads, hidden_channels, heads=heads, dropout=dropout))\n        # Output layer\n        if num_layers > 1:\n            # A common practice is to use a single head for the last layer to get a clean output dimension\n            self.convs.append(GATConv(hidden_channels * heads, hidden_channels, heads=1, dropout=dropout))\n            self.lin = torch.nn.Linear(hidden_channels, out_channels)\n        else:\n            self.lin = torch.nn.Linear(hidden_channels * heads, out_channels)\n            \n        self.dropout = dropout\n\n    def forward(self, data):\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n        for conv in self.convs[:-1]:\n            x = conv(x, edge_index)\n            x = F.elu(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n            \n        x = self.convs[-1](x, edge_index)\n        x = F.elu(x)\n        x = global_mean_pool(x, batch)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.lin(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T15:44:46.360639Z","iopub.execute_input":"2025-08-08T15:44:46.360926Z","iopub.status.idle":"2025-08-08T15:44:46.385176Z","shell.execute_reply.started":"2025-08-08T15:44:46.360900Z","shell.execute_reply":"2025-08-08T15:44:46.384310Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"def train_gnn(model, loader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0\n    for batch in loader:\n        batch = batch.to(device)\n        optimizer.zero_grad()\n        out = model(batch)\n        loss = criterion(out, batch.y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * batch.num_graphs\n    return total_loss / len(loader.dataset)\n\ndef test_gnn(model, loader, device):\n    model.eval()\n    correct = 0\n    for batch in loader:\n        batch = batch.to(device)\n        out = model(batch)\n        pred = out.argmax(dim=1)\n        correct += int((pred == batch.y).sum())\n    return correct / len(loader.dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T15:44:46.386206Z","iopub.execute_input":"2025-08-08T15:44:46.386465Z","iopub.status.idle":"2025-08-08T15:44:46.408901Z","shell.execute_reply.started":"2025-08-08T15:44:46.386445Z","shell.execute_reply":"2025-08-08T15:44:46.407956Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"class AddIdentityFeatures(BaseTransform):\n    def __call__(self, data):\n        if data.x is None:\n            data.x = torch.ones((data.num_nodes, 1))\n        return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T16:11:56.656446Z","iopub.execute_input":"2025-08-08T16:11:56.656827Z","iopub.status.idle":"2025-08-08T16:11:56.662284Z","shell.execute_reply.started":"2025-08-08T16:11:56.656800Z","shell.execute_reply":"2025-08-08T16:11:56.661311Z"}},"outputs":[],"execution_count":58},{"cell_type":"markdown","source":"Methods for the following filtrations and gnns for use in testing:","metadata":{}},{"cell_type":"code","source":"def run_experiment(dataset_name, filtrations, gnn_models, device='cpu'):\n    print(f\"Running experiment on dataset: {dataset_name}\")\n    dataset = TUDataset(root='/tmp/'+dataset_name, name=dataset_name, transform=AddIdentityFeatures())\n\n    # split train/test: 80/20\n    dataset = dataset.shuffle()\n    train_dataset = dataset[:int(0.8*len(dataset))]\n    test_dataset = dataset[int(0.8*len(dataset)):]\n\n    print(f\"Dataset size: {len(dataset)}, train: {len(train_dataset)}, test: {len(test_dataset)}\")\n\n    # XGBoost on Betti vectors\n    for filt_name, filt_fn in filtrations.items():\n        print(f\"\\nComputing Betti vectors with filtration: {filt_name}\")\n        X_train, y_train = get_betti_vectors(train_dataset, filt_fn)\n        X_test, y_test = get_betti_vectors(test_dataset, filt_fn)\n\n        if len(X_train) == 0:\n            print(f\"Skipping XGBoost for {filt_name} due to empty dataset.\")\n            continue\n            \n        model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        acc = accuracy_score(y_test, y_pred)\n        print(f\"XGBoost accuracy with {filt_name} filtration Betti vectors: {acc:.4f}\")\n\n    # GNN training\n    for model_name, model_fn in gnn_models.items():\n        print(f\"\\nTraining GNN model: {model_name}\")\n        # Fix: Get the correct number of node features\n        num_node_features = dataset[0].x.shape[1]\n        model = model_fn(num_node_features, 64, dataset.num_classes).to(device)\n        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n        criterion = torch.nn.CrossEntropyLoss()\n\n        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n        test_loader = DataLoader(test_dataset, batch_size=32)\n\n        for epoch in range(30):\n            loss = train_gnn(model, train_loader, optimizer, criterion, device)\n            acc = test_gnn(model, test_loader, device)\n            if epoch % 10 == 0 or epoch == 29:\n                print(f\"Epoch {epoch+1:02d}, Loss: {loss:.4f}, Test Acc: {acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T16:11:58.436578Z","iopub.execute_input":"2025-08-08T16:11:58.437479Z","iopub.status.idle":"2025-08-08T16:11:58.447516Z","shell.execute_reply.started":"2025-08-08T16:11:58.437439Z","shell.execute_reply":"2025-08-08T16:11:58.446504Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"filtrations = {\n    'degree': degree_filtration,\n    'clustering_coeff': clustering_coeff_filtration,\n    'pagerank': pagerank_filtration,\n    # 'heat_kernel': heat_kernel_filtration,\n}\n\ngnn_models = {\n    'GCN': GCN,\n    'GIN': GIN,\n    'GraphSAGE': GraphSAGE,\n    'GAT': GAT,\n}\n\ndatasets = [\n    'BZR',\n    'COX2',\n    'MUTAG',\n    'PROTEINS',\n    'IMDB-BINARY',\n    'IMDB-MULTI',\n    'REDDIT-BINARY',\n    'REDDIT-MULTI-5K'\n]\n\n# Use a smaller list of datasets for a quick run\n#filtrations = {}\n#datasets = ['REDDIT-BINARY']\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nfor ds in datasets:\n    run_experiment(ds, filtrations, gnn_models, device=device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T16:20:05.114909Z","iopub.execute_input":"2025-08-08T16:20:05.115846Z","iopub.status.idle":"2025-08-08T16:59:02.428987Z","shell.execute_reply.started":"2025-08-08T16:20:05.115815Z","shell.execute_reply":"2025-08-08T16:59:02.427902Z"}},"outputs":[{"name":"stdout","text":"Running experiment on dataset: BZR\nDataset size: 405, train: 324, test: 81\n\nComputing Betti vectors with filtration: degree\nXGBoost accuracy with degree filtration Betti vectors: 0.8025\n\nComputing Betti vectors with filtration: clustering_coeff\nXGBoost accuracy with clustering_coeff filtration Betti vectors: 0.7531\n\nComputing Betti vectors with filtration: pagerank\nXGBoost accuracy with pagerank filtration Betti vectors: 0.8148\n\nTraining GNN model: GCN\nEpoch 01, Loss: 0.5628, Test Acc: 0.7531\nEpoch 11, Loss: 0.4372, Test Acc: 0.7531\nEpoch 21, Loss: 0.4127, Test Acc: 0.7654\nEpoch 30, Loss: 0.4000, Test Acc: 0.7654\n\nTraining GNN model: GIN\nEpoch 01, Loss: 0.5780, Test Acc: 0.7531\nEpoch 11, Loss: 0.4253, Test Acc: 0.7531\nEpoch 21, Loss: 0.4197, Test Acc: 0.7531\nEpoch 30, Loss: 0.4341, Test Acc: 0.7531\n\nTraining GNN model: GraphSAGE\nEpoch 01, Loss: 0.5244, Test Acc: 0.7531\nEpoch 11, Loss: 0.4368, Test Acc: 0.7531\nEpoch 21, Loss: 0.4018, Test Acc: 0.7531\nEpoch 30, Loss: 0.4023, Test Acc: 0.7654\n\nTraining GNN model: GAT\nEpoch 01, Loss: 0.5883, Test Acc: 0.7531\nEpoch 11, Loss: 0.5033, Test Acc: 0.7531\nEpoch 21, Loss: 0.4762, Test Acc: 0.7531\nEpoch 30, Loss: 0.4547, Test Acc: 0.7654\nRunning experiment on dataset: COX2\nDataset size: 467, train: 373, test: 94\n\nComputing Betti vectors with filtration: degree\nXGBoost accuracy with degree filtration Betti vectors: 0.8085\n\nComputing Betti vectors with filtration: clustering_coeff\nXGBoost accuracy with clustering_coeff filtration Betti vectors: 0.7872\n\nComputing Betti vectors with filtration: pagerank\nXGBoost accuracy with pagerank filtration Betti vectors: 0.7660\n\nTraining GNN model: GCN\nEpoch 01, Loss: 0.5548, Test Acc: 0.7872\nEpoch 11, Loss: 0.5282, Test Acc: 0.7872\nEpoch 21, Loss: 0.5136, Test Acc: 0.7872\nEpoch 30, Loss: 0.5062, Test Acc: 0.7872\n\nTraining GNN model: GIN\nEpoch 01, Loss: 0.5616, Test Acc: 0.7872\nEpoch 11, Loss: 0.5244, Test Acc: 0.7872\nEpoch 21, Loss: 0.4959, Test Acc: 0.7872\nEpoch 30, Loss: 0.4942, Test Acc: 0.7872\n\nTraining GNN model: GraphSAGE\nEpoch 01, Loss: 0.5595, Test Acc: 0.7872\nEpoch 11, Loss: 0.5174, Test Acc: 0.7872\nEpoch 21, Loss: 0.5044, Test Acc: 0.7872\nEpoch 30, Loss: 0.5024, Test Acc: 0.7872\n\nTraining GNN model: GAT\nEpoch 01, Loss: 0.5957, Test Acc: 0.7872\nEpoch 11, Loss: 0.5415, Test Acc: 0.7872\nEpoch 21, Loss: 0.5345, Test Acc: 0.7872\nEpoch 30, Loss: 0.5257, Test Acc: 0.7872\nRunning experiment on dataset: MUTAG\nDataset size: 188, train: 150, test: 38\n\nComputing Betti vectors with filtration: degree\nXGBoost accuracy with degree filtration Betti vectors: 0.8947\n\nComputing Betti vectors with filtration: clustering_coeff\nXGBoost accuracy with clustering_coeff filtration Betti vectors: 0.8684\n\nComputing Betti vectors with filtration: pagerank\nXGBoost accuracy with pagerank filtration Betti vectors: 0.8684\n\nTraining GNN model: GCN\nEpoch 01, Loss: 0.6827, Test Acc: 0.8684\nEpoch 11, Loss: 0.5864, Test Acc: 0.7632\nEpoch 21, Loss: 0.5801, Test Acc: 0.8158\nEpoch 30, Loss: 0.5408, Test Acc: 0.7632\n\nTraining GNN model: GIN\nEpoch 01, Loss: 0.6945, Test Acc: 0.8684\nEpoch 11, Loss: 0.5543, Test Acc: 0.7632\nEpoch 21, Loss: 0.5061, Test Acc: 0.7632\nEpoch 30, Loss: 0.5221, Test Acc: 0.8421\n\nTraining GNN model: GraphSAGE\nEpoch 01, Loss: 0.6849, Test Acc: 0.8684\nEpoch 11, Loss: 0.5618, Test Acc: 0.8158\nEpoch 21, Loss: 0.5152, Test Acc: 0.8158\nEpoch 30, Loss: 0.5204, Test Acc: 0.8158\n\nTraining GNN model: GAT\nEpoch 01, Loss: 0.6865, Test Acc: 0.8158\nEpoch 11, Loss: 0.5812, Test Acc: 0.7895\nEpoch 21, Loss: 0.5672, Test Acc: 0.7895\nEpoch 30, Loss: 0.5656, Test Acc: 0.7895\nRunning experiment on dataset: PROTEINS\nDataset size: 1113, train: 890, test: 223\n\nComputing Betti vectors with filtration: degree\nXGBoost accuracy with degree filtration Betti vectors: 0.6682\n\nComputing Betti vectors with filtration: clustering_coeff\nXGBoost accuracy with clustering_coeff filtration Betti vectors: 0.6996\n\nComputing Betti vectors with filtration: pagerank\nXGBoost accuracy with pagerank filtration Betti vectors: 0.7130\n\nTraining GNN model: GCN\nEpoch 01, Loss: 0.6935, Test Acc: 0.6009\nEpoch 11, Loss: 0.6454, Test Acc: 0.6323\nEpoch 21, Loss: 0.6298, Test Acc: 0.6951\nEpoch 30, Loss: 0.6339, Test Acc: 0.6861\n\nTraining GNN model: GIN\nEpoch 01, Loss: 0.7006, Test Acc: 0.6009\nEpoch 11, Loss: 0.6159, Test Acc: 0.6996\nEpoch 21, Loss: 0.6091, Test Acc: 0.6951\nEpoch 30, Loss: 0.6200, Test Acc: 0.6951\n\nTraining GNN model: GraphSAGE\nEpoch 01, Loss: 0.6926, Test Acc: 0.6323\nEpoch 11, Loss: 0.6358, Test Acc: 0.6368\nEpoch 21, Loss: 0.6227, Test Acc: 0.7040\nEpoch 30, Loss: 0.6152, Test Acc: 0.6816\n\nTraining GNN model: GAT\nEpoch 01, Loss: 0.6938, Test Acc: 0.6368\nEpoch 11, Loss: 0.6711, Test Acc: 0.6099\nEpoch 21, Loss: 0.6767, Test Acc: 0.3991\nEpoch 30, Loss: 0.6923, Test Acc: 0.6233\nRunning experiment on dataset: IMDB-BINARY\nDataset size: 1000, train: 800, test: 200\n\nComputing Betti vectors with filtration: degree\nXGBoost accuracy with degree filtration Betti vectors: 0.6850\n\nComputing Betti vectors with filtration: clustering_coeff\nXGBoost accuracy with clustering_coeff filtration Betti vectors: 0.4700\n\nComputing Betti vectors with filtration: pagerank\nXGBoost accuracy with pagerank filtration Betti vectors: 0.6400\n\nTraining GNN model: GCN\nEpoch 01, Loss: 0.6978, Test Acc: 0.4700\nEpoch 11, Loss: 0.6947, Test Acc: 0.4700\nEpoch 21, Loss: 0.6935, Test Acc: 0.5300\nEpoch 30, Loss: 0.6931, Test Acc: 0.4700\n\nTraining GNN model: GIN\nEpoch 01, Loss: 1.4130, Test Acc: 0.5950\nEpoch 11, Loss: 0.6795, Test Acc: 0.6200\nEpoch 21, Loss: 0.6750, Test Acc: 0.6600\nEpoch 30, Loss: 0.6688, Test Acc: 0.6650\n\nTraining GNN model: GraphSAGE\nEpoch 01, Loss: 0.7274, Test Acc: 0.5300\nEpoch 11, Loss: 0.6935, Test Acc: 0.4700\nEpoch 21, Loss: 0.6933, Test Acc: 0.4700\nEpoch 30, Loss: 0.6934, Test Acc: 0.4700\n\nTraining GNN model: GAT\nEpoch 01, Loss: 0.7315, Test Acc: 0.5300\nEpoch 11, Loss: 0.7063, Test Acc: 0.4700\nEpoch 21, Loss: 0.7111, Test Acc: 0.4700\nEpoch 30, Loss: 0.7216, Test Acc: 0.5300\nRunning experiment on dataset: IMDB-MULTI\n","output_type":"stream"},{"name":"stderr","text":"Downloading https://www.chrsmrrs.com/graphkerneldatasets/IMDB-MULTI.zip\nProcessing...\nDone!\n","output_type":"stream"},{"name":"stdout","text":"Dataset size: 1500, train: 1200, test: 300\n\nComputing Betti vectors with filtration: degree\nXGBoost accuracy with degree filtration Betti vectors: 0.3800\n\nComputing Betti vectors with filtration: clustering_coeff\nXGBoost accuracy with clustering_coeff filtration Betti vectors: 0.3033\n\nComputing Betti vectors with filtration: pagerank\nXGBoost accuracy with pagerank filtration Betti vectors: 0.4233\n\nTraining GNN model: GCN\nEpoch 01, Loss: 1.1039, Test Acc: 0.3500\nEpoch 11, Loss: 1.0991, Test Acc: 0.3033\nEpoch 21, Loss: 1.0990, Test Acc: 0.3500\nEpoch 30, Loss: 1.0988, Test Acc: 0.3033\n\nTraining GNN model: GIN\nEpoch 01, Loss: 1.3705, Test Acc: 0.3033\nEpoch 11, Loss: 1.0996, Test Acc: 0.3500\nEpoch 21, Loss: 1.1000, Test Acc: 0.3033\nEpoch 30, Loss: 1.0995, Test Acc: 0.3033\n\nTraining GNN model: GraphSAGE\nEpoch 01, Loss: 1.1240, Test Acc: 0.3500\nEpoch 11, Loss: 1.0997, Test Acc: 0.3033\nEpoch 21, Loss: 1.1002, Test Acc: 0.3033\nEpoch 30, Loss: 1.0993, Test Acc: 0.3467\n\nTraining GNN model: GAT\nEpoch 01, Loss: 1.1156, Test Acc: 0.3500\nEpoch 11, Loss: 1.1414, Test Acc: 0.3033\nEpoch 21, Loss: 1.1274, Test Acc: 0.3033\nEpoch 30, Loss: 1.1222, Test Acc: 0.3033\nRunning experiment on dataset: REDDIT-BINARY\nDataset size: 2000, train: 1600, test: 400\n\nComputing Betti vectors with filtration: degree\nXGBoost accuracy with degree filtration Betti vectors: 0.8625\n\nComputing Betti vectors with filtration: clustering_coeff\nXGBoost accuracy with clustering_coeff filtration Betti vectors: 0.7925\n\nComputing Betti vectors with filtration: pagerank\nXGBoost accuracy with pagerank filtration Betti vectors: 0.8300\n\nTraining GNN model: GCN\nEpoch 01, Loss: 0.6964, Test Acc: 0.4775\nEpoch 11, Loss: 0.6132, Test Acc: 0.6900\nEpoch 21, Loss: 0.5999, Test Acc: 0.6775\nEpoch 30, Loss: 0.6132, Test Acc: 0.6750\n\nTraining GNN model: GIN\nEpoch 01, Loss: 1.0241, Test Acc: 0.7075\nEpoch 11, Loss: 0.6806, Test Acc: 0.6050\nEpoch 21, Loss: 0.6669, Test Acc: 0.6450\nEpoch 30, Loss: 0.6406, Test Acc: 0.7150\n\nTraining GNN model: GraphSAGE\nEpoch 01, Loss: 0.7065, Test Acc: 0.4775\nEpoch 11, Loss: 0.6936, Test Acc: 0.4775\nEpoch 21, Loss: 0.6937, Test Acc: 0.4775\nEpoch 30, Loss: 0.6941, Test Acc: 0.5225\n\nTraining GNN model: GAT\nEpoch 01, Loss: 0.7127, Test Acc: 0.5225\nEpoch 11, Loss: 0.6933, Test Acc: 0.5225\nEpoch 21, Loss: 0.6949, Test Acc: 0.5225\nEpoch 30, Loss: 0.6981, Test Acc: 0.5225\nRunning experiment on dataset: REDDIT-MULTI-5K\n","output_type":"stream"},{"name":"stderr","text":"Downloading https://www.chrsmrrs.com/graphkerneldatasets/REDDIT-MULTI-5K.zip\nProcessing...\nDone!\n","output_type":"stream"},{"name":"stdout","text":"Dataset size: 4999, train: 3999, test: 1000\n\nComputing Betti vectors with filtration: degree\nXGBoost accuracy with degree filtration Betti vectors: 0.4350\n\nComputing Betti vectors with filtration: clustering_coeff\nXGBoost accuracy with clustering_coeff filtration Betti vectors: 0.4520\n\nComputing Betti vectors with filtration: pagerank\nXGBoost accuracy with pagerank filtration Betti vectors: 0.4310\n\nTraining GNN model: GCN\nEpoch 01, Loss: 1.6128, Test Acc: 0.1960\nEpoch 11, Loss: 1.5516, Test Acc: 0.3750\nEpoch 21, Loss: 1.5188, Test Acc: 0.4520\nEpoch 30, Loss: 1.4943, Test Acc: 0.4060\n\nTraining GNN model: GIN\nEpoch 01, Loss: 1.6523, Test Acc: 0.1780\nEpoch 11, Loss: 1.6103, Test Acc: 0.2000\nEpoch 21, Loss: 1.6104, Test Acc: 0.1780\nEpoch 30, Loss: 1.6105, Test Acc: 0.1960\n\nTraining GNN model: GraphSAGE\nEpoch 01, Loss: 1.6174, Test Acc: 0.1780\nEpoch 11, Loss: 1.6106, Test Acc: 0.1780\nEpoch 21, Loss: 1.6098, Test Acc: 0.1800\nEpoch 30, Loss: 1.6099, Test Acc: 0.1810\n\nTraining GNN model: GAT\nEpoch 01, Loss: 1.6294, Test Acc: 0.1780\nEpoch 11, Loss: 1.6208, Test Acc: 0.2180\nEpoch 21, Loss: 1.6172, Test Acc: 0.2180\nEpoch 30, Loss: 1.6165, Test Acc: 0.2180\n","output_type":"stream"}],"execution_count":61}]}