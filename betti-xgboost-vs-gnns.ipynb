{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install torch\n!pip install torch-geometric                      \n!pip install ripser                               \n!pip install networkx                             \n!pip install scikit-learn                         \n!pip install gudhi\n!pip install giotto-tda\n!pip install networkx","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-08T15:02:19.556156Z","iopub.execute_input":"2025-08-08T15:02:19.556450Z","iopub.status.idle":"2025-08-08T15:02:50.530273Z","shell.execute_reply.started":"2025-08-08T15:02:19.556422Z","shell.execute_reply":"2025-08-08T15:02:50.529114Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.datasets import TUDataset\nfrom torch_geometric.loader import DataLoader\nfrom torch_geometric.nn import GCNConv, GINConv, SAGEConv, GATConv, global_mean_pool\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport xgboost as xgb\nfrom gudhi import SimplexTree\nimport networkx as nx\nimport gtda.homology as gph\nfrom gtda.diagrams import BettiCurve\nimport scipy\nimport scipy.sparse.linalg\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-08T14:59:05.612Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===== FILTRATION METHODS =====\n\n# helper\ndef to_networkx(data):\n    G = nx.Graph()\n    edge_index = data.edge_index.numpy()\n    edges = list(zip(edge_index[0], edge_index[1]))\n    G.add_edges_from(edges)\n    return G\n\ndef degree_filtration(data):\n    deg = torch.bincount(data.edge_index[0], minlength=data.num_nodes)\n    return deg.numpy()\n\n\ndef clustering_coeff_filtration(data):\n    G = to_networkx(data)\n    clust = nx.clustering(G)\n    coeffs = np.array([clust.get(i, 0) for i in range(data.num_nodes)])\n    return coeffs\n\ndef pagerank_filtration(data):\n    G = to_networkx(data)\n    pr = nx.pagerank(G)\n    pr_vals = np.array([pr.get(i, 0) for i in range(data.num_nodes)])\n    return pr_vals\n\ndef heat_kernel_filtration(data, t=1.0):\n    G = to_networkx(data)\n    L = nx.normalized_laplacian_matrix(G)\n    try:\n        eigvals, eigvecs = scipy.sparse.linalg.eigsh(L, k=min(100, data.num_nodes - 1), which='SM')\n    except:\n        L_dense = L.todense()\n        eigvals, eigvecs = np.linalg.eigh(L_dense)\n    diag = np.sum((eigvecs ** 2) * np.exp(-t * eigvals)[None, :], axis=1)\n    return diag","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-08T14:59:05.613Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_simplex_tree(data, filtration_values):\n    st = SimplexTree()\n    for i, fval in enumerate(filtration_values):\n        st.insert([i], filtration=fval)\n    edges = data.edge_index.t().numpy()\n    for u, v in edges:\n        fval = max(filtration_values[u], filtration_values[v])\n        st.insert([u, v], filtration=fval)\n    return st\n\ndef compute_betti_curve(st, dim=0, n_bins=100):\n    st.compute_persistence()\n    diag = np.array(st.persistence_intervals_in_dimension(dim))\n    if diag.size > 0:\n        max_finite = np.max(diag[np.isfinite(diag)])\n        diag[np.isinf(diag)] = max_finite + 1\n        dim_col = np.full((diag.shape[0], 1), fill_value=dim)\n        diag_3d = np.hstack([diag, dim_col])\n    else:\n        # Handle empty diagrams gracefully (no intervals)\n        diag_3d = np.empty((0, 3))\n    bc = BettiCurve(n_bins=n_bins)\n    betti_vector = bc.fit_transform([diag_3d])[0]\n    return betti_vector\n\n\n\ndef compute_betti_vector(data, filtration_fn):\n    filt_vals = filtration_fn(data)\n    st = build_simplex_tree(data, filt_vals)\n    return compute_betti_curve(st, dim=0)\n\ndef get_betti_vectors(dataset, filtration_fn):\n    X, y = [], []\n    for data in dataset:\n        bv = compute_betti_vector(data, filtration_fn)\n        bv = bv.flatten()  # ensure it's 1D\n        X.append(bv)\n        label = data.y.item() if data.y.dim() == 0 else data.y.numpy()\n        y.append(label)\n    X = np.vstack(X)  # ensures 2D shape for X\n    y = np.array(y)\n    return X, y","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-08T14:59:05.613Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class GCN(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, dropout=0.5):\n        super().__init__()\n        self.convs = torch.nn.ModuleList()\n        self.convs.append(GCNConv(in_channels, hidden_channels))\n        for _ in range(num_layers - 2):\n            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n        if num_layers > 1:\n            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n        self.lin = torch.nn.Linear(hidden_channels, out_channels)\n        self.dropout = dropout\n\n    def forward(self, data):\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n        for conv in self.convs[:-1]:\n            x = conv(x, edge_index)\n            x = F.relu(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.convs[-1](x, edge_index)\n        x = F.relu(x)\n        x = global_mean_pool(x, batch)\n        x = self.lin(x)\n        return x\n\nclass GIN(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, dropout=0.5):\n        super().__init__()\n        self.convs = torch.nn.ModuleList()\n        for layer in range(num_layers):\n            if layer == 0:\n                nn = torch.nn.Sequential(\n                    torch.nn.Linear(in_channels, hidden_channels),\n                    torch.nn.ReLU(),\n                    torch.nn.Linear(hidden_channels, hidden_channels),\n                )\n            else:\n                nn = torch.nn.Sequential(\n                    torch.nn.Linear(hidden_channels, hidden_channels),\n                    torch.nn.ReLU(),\n                    torch.nn.Linear(hidden_channels, hidden_channels),\n                )\n            self.convs.append(GINConv(nn))\n        self.lin = torch.nn.Linear(hidden_channels, out_channels)\n        self.dropout = dropout\n\n    def forward(self, data):\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n        for conv in self.convs[:-1]:\n            x = conv(x, edge_index)\n            x = F.relu(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.convs[-1](x, edge_index)\n        x = F.relu(x)\n        x = global_mean_pool(x, batch)\n        x = self.lin(x)\n        return x\n\nclass GraphSAGE(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, dropout=0.5):\n        super().__init__()\n        self.convs = torch.nn.ModuleList()\n        self.convs.append(SAGEConv(in_channels, hidden_channels))\n        for _ in range(num_layers - 2):\n            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n        if num_layers > 1:\n            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n        self.lin = torch.nn.Linear(hidden_channels, out_channels)\n        self.dropout = dropout\n\n    def forward(self, data):\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n        for conv in self.convs[:-1]:\n            x = conv(x, edge_index)\n            x = F.relu(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.convs[-1](x, edge_index)\n        x = F.relu(x)\n        x = global_mean_pool(x, batch)\n        x = self.lin(x)\n        return x\n\nclass GAT(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, heads=4, dropout=0.6):\n        super().__init__()\n        self.convs = torch.nn.ModuleList()\n        self.convs.append(GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout))\n        for _ in range(num_layers - 2):\n            self.convs.append(GATConv(hidden_channels * heads, hidden_channels, heads=heads, dropout=dropout))\n        if num_layers > 1:\n            self.convs.append(GATConv(hidden_channels * heads, hidden_channels, heads=heads, dropout=dropout))\n        self.lin = torch.nn.Linear(hidden_channels * heads, out_channels)\n        self.dropout = dropout\n\n    def forward(self, data):\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n        for conv in self.convs[:-1]:\n            x = conv(x, edge_index)\n            x = F.elu(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.convs[-1](x, edge_index)\n        x = F.elu(x)\n        x = global_mean_pool(x, batch)\n        x = self.lin(x)\n        return x","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-08T14:59:05.613Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_gnn(model, loader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0\n    for batch in loader:\n        batch = batch.to(device)\n        optimizer.zero_grad()\n        out = model(batch)\n        loss = criterion(out, batch.y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * batch.num_graphs\n    return total_loss / len(loader.dataset)\n\ndef test_gnn(model, loader, device):\n    model.eval()\n    correct = 0\n    for batch in loader:\n        batch = batch.to(device)\n        out = model(batch)\n        pred = out.argmax(dim=1)\n        correct += int((pred == batch.y).sum())\n    return correct / len(loader.dataset)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-08T14:59:05.613Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Methods for the following filtrations and gnns for use in testing:","metadata":{}},{"cell_type":"code","source":"def run_experiment(dataset_name, filtrations, gnn_models, device='cpu'):\n    print(f\"Running experiment on dataset: {dataset_name}\")\n    dataset = TUDataset(root='/tmp/'+dataset_name, name=dataset_name)\n    # assign identity matrix for node features if they aren't provided (ex. for IMDB and Reddit datasets)\n    if dataset.num_node_features==0:\n        for data in dataset:\n            data.x = torch.eye(data.num_nodes)\n\n    # split train/test: 80/20\n    dataset = dataset.shuffle()\n    train_dataset = dataset[:int(0.8*len(dataset))]\n    test_dataset = dataset[int(0.8*len(dataset)):]\n\n    print(f\"Dataset size: {len(dataset)}, train: {len(train_dataset)}, test: {len(test_dataset)}\")\n\n    # XGBoost on Betti vectors\n    for filt_name, filt_fn in filtrations.items():\n        print(f\"\\nComputing Betti vectors with filtration: {filt_name}\")\n        X_train, y_train = get_betti_vectors(train_dataset, filt_fn)\n        X_test, y_test = get_betti_vectors(test_dataset, filt_fn)\n\n        model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        acc = accuracy_score(y_test, y_pred)\n        print(f\"XGBoost accuracy with {filt_name} filtration Betti vectors: {acc:.4f}\")\n\n    # GNN training\n    for model_name,  model_fn in gnn_models.items():\n        print(f\"\\nTraining GNN model: {model_name}\")\n        model = model_fn(dataset.num_node_features, 64, dataset.num_classes).to(device)\n        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n        criterion = torch.nn.CrossEntropyLoss()\n\n        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n        test_loader = DataLoader(test_dataset, batch_size=32)\n\n        for epoch in range(30):\n            loss = train_gnn(model, train_loader, optimizer, criterion, device)\n            acc = test_gnn(model, test_loader, device)\n            if epoch % 10 == 0 or epoch == 29:\n                print(f\"Epoch {epoch+1:02d}, Loss: {loss:.4f}, Test Acc: {acc:.4f}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-08T14:59:05.613Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"filtrations = {\n    'degree': degree_filtration,\n    'clustering_coeff': clustering_coeff_filtration,\n    'pagerank': pagerank_filtration,\n    'heat_kernel': heat_kernel_filtration,\n}\n\ngnn_models = {\n    'GCN': GCN,\n    'GIN': GIN,\n    'GraphSAGE': GraphSAGE,\n    'GAT': GAT,\n}\n\ndatasets = [\n    'BZR',\n    'COX2',\n    'MUTAG',\n    'PROTEINS',\n    'IMDB-BINARY',\n    'IMDB-MULTI'\n    'REDDIT-BINARY',\n    'REDDIT-MULTI-5K'\n]\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nfor ds in datasets:\n    run_experiment(ds, filtrations, gnn_models, device=device)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-08T14:59:05.613Z"}},"outputs":[],"execution_count":null}]}